+ Project goals:
    - Sparkify - A music streaming startup, has grown the user base and song database even more and want to change the way data stored in the data warehouse into a data lake. 
    - After completing the project, the analytics team is planned to discover the valuable insights in the songs that their users are pay attention to.
    
+ Current Criteria :
    - The data resided in S3 where a directory of JSON logs on user activity on the app stored, as well as a directory with JSON metadata on the songs in this app.

+ Project's materials :
    - Song Data: "s3://udacity-dend/song_data" is the subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. (partitioned by the first three letters of each song's track ID)
    - Log Data: "s3://udacity-dend/log_data" is the second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. (partitioned by year and month)
    - Fact Table : songplays - records in log data associated with song plays i.e. records with page "NextSong".
    - Dimension Table : 
        .Table "users" - users in the app.
        .Table "songs" - songs in music database.
        .Table "artists" - artists in music database.
        .Table "time" - timestamps of records in songplays broken down into specific units.
    - File "etl.py" reads data from S3, processes that data using Spark, and writes them back to AWS S3.
    - File "dl.cfg" contains the setting of AWS credentials.
    - File "README.md" provides discussion on the process and decisions. 
    
+ Requirements:
    - Build the ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables.
    - Required to load data from S3, then process the data into analytics tables using Spark, and load them back into S3.
    - After that, implement the deployment the Spark process on a cluster using AWS.

+ Implementing steps:
    - Firstly, Implement Configuration of the cluster with the essential settings.
    - Secondly, Just Waiting for the status "Waiting" of the Cluster.
    - Thirdly, Create the Notebook to run Spark on that cluster.
    - Then, Configure the created notebook on above step with the needed settings (Pay attention to the setting for "AWS service role").
    - Finally, just Wait for the status "Ready" of the Notebook. (Processign with the EMR running on the cluster and take a look at the EMR cluster's price).
    
+ ETL Process : 
    - Build an ETL pipeline using Python then reading data from log files from directories data\song_data and data\log_data and make Data Lack by saving tables in parquet format in s3 .

+ Project files : 
    - File "etl.py" reads and processes files from song_data and log_data and make Data Lack by saving tables in parquet format in s3
    - File "README.md" provides discussion on your project.
    
+ How To Run the Project :
    - Build ETL Processes.
    - Run File "etl.py" to build ETL process.
